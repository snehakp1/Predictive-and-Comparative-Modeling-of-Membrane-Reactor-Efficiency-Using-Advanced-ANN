{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE3Qh5Ix0Q2b"
      },
      "source": [
        "## Hyperparameter tuning - Random Forest , XGBoost, LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gm4ziEQzSRs",
        "outputId": "898fd8f4-9da4-4a2c-96e5-ea7a8964287b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'hydrogen (Python 3.11.13)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n hydrogen ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time # To time operations\n",
        "\n",
        "# --- ML Models ---\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# --- Preprocessing & Splitting ---\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Metrics & Utilities ---\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tqdm import tqdm # Keep progress bar support if needed (though GridSearchCV has its own)\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# Filter convergence warnings for cleaner output if desired\n",
        "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ensure reproducibility\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# --- Data Loading and Preprocessing (Same as before) ---\n",
        "def load_and_preprocess_data(file_path, test_size=0.2, random_state=42):\n",
        "    \"\"\"Loads data, scales features, and splits into train and test sets.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        print(\"Creating dummy data for demonstration.\")\n",
        "        # Dummy data generation needs feature names if columns are expected later\n",
        "        num_features = 9 # Assuming 9 features based on previous context\n",
        "        df = pd.DataFrame({\n",
        "            f'feature{i+1}': np.random.rand(200) for i in range(num_features)\n",
        "        })\n",
        "        # Example dummy target relationship\n",
        "        df['efficiency'] = (df['feature1'] * 50 + df['feature2'] * 30 +\n",
        "                            np.random.normal(0, 5, 200)) # Added some noise\n",
        "\n",
        "    if 'efficiency' not in df.columns:\n",
        "        raise ValueError(\"Column 'efficiency' not found in the dataframe.\")\n",
        "\n",
        "    X = df.drop('efficiency', axis=1)\n",
        "    y = df['efficiency']\n",
        "\n",
        "    # Basic check for NaN/Infinite values\n",
        "    if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:\n",
        "        print(\"Warning: NaN values found in data. Consider imputation.\")\n",
        "        # Example: X = X.fillna(X.median()) # Use median or other strategy\n",
        "        #          y = y.fillna(y.median())\n",
        "    if np.isinf(X.values).sum() > 0 or np.isinf(y.values).sum() > 0:\n",
        "         print(\"Warning: Infinite values found in data. Consider handling.\")\n",
        "         # Example: X.replace([np.inf, -np.inf], np.nan, inplace=True) then impute\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Ensure y are numpy arrays\n",
        "    y_train = y_train.to_numpy() if isinstance(y_train, pd.Series) else np.asarray(y_train)\n",
        "    y_test = y_test.to_numpy() if isinstance(y_test, pd.Series) else np.asarray(y_test)\n",
        "\n",
        "    print(f\"Data shapes: Train (X:{X_train.shape}, y:{y_train.shape}), Test (X:{X_test.shape}, y:{y_test.shape})\")\n",
        "    return (X_train, y_train), (X_test, y_test), scaler\n",
        "\n",
        "\n",
        "# --- GridSearchCV Tuning Function (Same as before) ---\n",
        "def tune_model_gridsearch(model, param_grid, X_train, y_train, n_splits=5, scoring='r2'):\n",
        "    \"\"\"Tunes hyperparameters using GridSearchCV with K-Fold CV.\"\"\"\n",
        "    model_name = model.__class__.__name__\n",
        "    print(f\"\\n--- Tuning {model_name} using GridSearchCV ---\")\n",
        "    print(f\"Parameter Grid: {param_grid}\")\n",
        "    print(f\"CV Folds (k): {n_splits}\")\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    # Add verbosity control for LightGBM\n",
        "    fit_params = {}\n",
        "    if isinstance(model, lgb.LGBMRegressor):\n",
        "        # Suppress verbose logging during CV fits for LGBM\n",
        "        fit_params['callbacks'] = [lgb.log_evaluation(period=0)]\n",
        "\n",
        "    start_time = time.time()\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        scoring=scoring,\n",
        "        cv=kf,\n",
        "        n_jobs=-1, # Use all available CPU cores\n",
        "        verbose=1  # Show progress from GridSearchCV\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train, **fit_params)\n",
        "\n",
        "    runtime = time.time() - start_time\n",
        "    print(f\"{model_name} tuning finished in {runtime:.2f} seconds.\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    # Ensure score is extracted correctly, handle potential issues if fit failed\n",
        "    best_score = grid_search.best_score_ if hasattr(grid_search, 'best_score_') else np.nan\n",
        "    print(f\"Best CV Score ({scoring}): {best_score:.6f}\")\n",
        "\n",
        "    # Return the best model found by GridSearchCV (already refit on full training data)\n",
        "    return grid_search.best_estimator_, best_score\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    # --- Configuration ---\n",
        "    file_path = 'data_set.csv' # <--- *** UPDATE THIS PATH ***\n",
        "    test_set_size = 0.2\n",
        "    num_cv_folds_tuning = 5 # Folds for GridSearchCV\n",
        "\n",
        "    # Flags to enable/disable model tuning/evaluation\n",
        "    # Set flags for the models you want to run\n",
        "    run_random_forest = True\n",
        "    run_xgboost = True\n",
        "    run_lightgbm = True\n",
        "\n",
        "    # GridSearchCV Parameter Grids (Adjust these based on desired search complexity/time)\n",
        "    # Smaller grids run faster\n",
        "    rf_param_grid = {\n",
        "        'n_estimators': [100, 200],        # Number of trees\n",
        "        'max_depth': [10, 20, None],       # Max depth of trees (None=unlimited)\n",
        "        'min_samples_split': [2, 5],       # Min samples to split a node\n",
        "        'min_samples_leaf': [1, 3],        # Min samples in a leaf node\n",
        "        'max_features': ['sqrt', 1.0]      # Features to consider for split ('sqrt', 'log2', float fraction, or 1.0 for all in recent sklearn)\n",
        "    }\n",
        "\n",
        "    xgb_param_grid = {\n",
        "        'n_estimators': [100, 200],        # Number of boosting rounds\n",
        "        'max_depth': [3, 5, 7],            # Max depth per tree\n",
        "        'learning_rate': [0.05, 0.1],      # Step size shrinkage\n",
        "        'subsample': [0.7, 0.9, 1.0],        # Fraction of samples used per tree\n",
        "        'colsample_bytree': [0.7, 0.9, 1.0],   # Fraction of features used per tree\n",
        "        # Add other parameters like gamma, reg_alpha, reg_lambda if needed\n",
        "    }\n",
        "\n",
        "    lgbm_param_grid = {\n",
        "        'n_estimators': [100, 200],        # Number of boosting rounds\n",
        "        'max_depth': [5, 10, -1],          # Max depth (-1 = no limit)\n",
        "        'learning_rate': [0.05, 0.1],      # Step size shrinkage\n",
        "        'num_leaves': [20, 31, 40],        # Max leaves in one tree (key param for LGBM)\n",
        "        'subsample': [0.7, 0.9, 1.0],        # Fraction of samples used per tree\n",
        "        'colsample_bytree': [0.7, 0.9, 1.0],   # Fraction of features used per tree\n",
        "         # Add other parameters like reg_alpha, reg_lambda if needed\n",
        "    }\n",
        "\n",
        "    # --- Data Loading ---\n",
        "    try:\n",
        "        (X_train, y_train), (X_test, y_test), scaler = load_and_preprocess_data(\n",
        "            file_path, test_size=test_set_size, random_state=seed\n",
        "        )\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        print(f\"Error loading or processing data: {e}\"); return\n",
        "\n",
        "    # Dictionary to store results\n",
        "    best_models = {}\n",
        "    cv_scores = {}\n",
        "    test_results = {}\n",
        "\n",
        "    # --- Random Forest Tuning (Optional) ---\n",
        "    if run_random_forest:\n",
        "        # Initialize model with random state for reproducibility\n",
        "        rf_model = RandomForestRegressor(random_state=seed, n_jobs=1) # n_jobs=1 here if GridSearchCV uses -1\n",
        "        best_rf_model, best_rf_cv_score = tune_model_gridsearch(\n",
        "            rf_model, rf_param_grid, X_train, y_train, n_splits=num_cv_folds_tuning, scoring='r2'\n",
        "        )\n",
        "        best_models['RandomForest'] = best_rf_model\n",
        "        cv_scores['RandomForest'] = best_rf_cv_score\n",
        "\n",
        "    # --- XGBoost Tuning (Optional) ---\n",
        "    if run_xgboost:\n",
        "        # Specify objective for regression and other base parameters\n",
        "        # Set verbosity=0 to suppress XGBoost's own messages during CV\n",
        "        xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=seed, verbosity=0, n_jobs=1)\n",
        "        best_xgb_model, best_xgb_cv_score = tune_model_gridsearch(\n",
        "            xgb_model, xgb_param_grid, X_train, y_train, n_splits=num_cv_folds_tuning, scoring='r2'\n",
        "        )\n",
        "        best_models['XGBoost'] = best_xgb_model\n",
        "        cv_scores['XGBoost'] = best_xgb_cv_score\n",
        "\n",
        "    # --- LightGBM Tuning (Optional) ---\n",
        "    if run_lightgbm:\n",
        "        # Specify objective, metric, and control verbosity\n",
        "        lgbm_model = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=seed, verbosity=-1, n_jobs=1)\n",
        "        best_lgbm_model, best_lgbm_cv_score = tune_model_gridsearch(\n",
        "            lgbm_model, lgbm_param_grid, X_train, y_train, n_splits=num_cv_folds_tuning, scoring='r2'\n",
        "        )\n",
        "        best_models['LightGBM'] = best_lgbm_model\n",
        "        cv_scores['LightGBM'] = best_lgbm_cv_score\n",
        "\n",
        "    # --- Final Evaluation on Test Set ---\n",
        "    print(\"\\n--- Final Model Evaluation on Test Set ---\")\n",
        "    if not best_models:\n",
        "        print(\"No models were successfully tuned.\")\n",
        "        return\n",
        "\n",
        "    for name, model in best_models.items():\n",
        "        try:\n",
        "            start_predict_time = time.time()\n",
        "            y_pred_test = model.predict(X_test)\n",
        "            predict_time = time.time() - start_predict_time\n",
        "\n",
        "            test_r2 = r2_score(y_test, y_pred_test)\n",
        "            test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "            test_results[name] = {\n",
        "                'Test_R2': test_r2,\n",
        "                'Test_MSE': test_mse,\n",
        "                'Best_CV_R2': cv_scores.get(name, np.nan),\n",
        "                'Predict_Time_s': predict_time\n",
        "            }\n",
        "            print(f\"{name}:\")\n",
        "            print(f\"  Test R2:  {test_r2:.6f}\")\n",
        "            print(f\"  Test MSE: {test_mse:.6f}\")\n",
        "            print(f\"  Best CV R2: {cv_scores.get(name, np.nan):.6f}\")\n",
        "            print(f\"  Prediction Time: {predict_time:.4f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {name} on test set: {e}\")\n",
        "            test_results[name] = {\n",
        "                'Test_R2': np.nan, 'Test_MSE': np.nan,\n",
        "                'Best_CV_R2': cv_scores.get(name, np.nan), 'Predict_Time_s': np.nan\n",
        "            }\n",
        "\n",
        "    # --- Optional: Add Comparative Visualization ---\n",
        "    if test_results:\n",
        "        print(\"\\n--- Results Summary ---\")\n",
        "        results_df = pd.DataFrame(test_results).T # Transpose for better table format\n",
        "        # Ensure consistent column order\n",
        "        results_df = results_df[['Best_CV_R2', 'Test_R2', 'Test_MSE', 'Predict_Time_s']]\n",
        "        print(results_df.sort_values(by='Test_R2', ascending=False))\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        models_evaluated = list(results_df.index)\n",
        "        test_r2_scores = results_df['Test_R2'].fillna(-1).values # Handle potential NaN for plotting\n",
        "\n",
        "        # Define colors - ensure enough colors if more models are added\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "        bars = plt.bar(models_evaluated, test_r2_scores, color=colors[:len(models_evaluated)])\n",
        "\n",
        "        plt.ylabel('Test R2 Score')\n",
        "        plt.title('Comparison of Tuned Models on Test Set (R2 Score)')\n",
        "        # Adjust ylim dynamically based on scores\n",
        "        min_score = results_df['Test_R2'].min()\n",
        "        max_score = results_df['Test_R2'].max()\n",
        "        lower_lim = max(0, min_score - 0.05) if pd.notna(min_score) else 0\n",
        "        upper_lim = min(1.05, max_score + 0.05) if pd.notna(max_score) else 1.05\n",
        "        plt.ylim(bottom=lower_lim, top=upper_lim)\n",
        "\n",
        "        # Add score labels on top of bars\n",
        "        for bar in bars:\n",
        "            yval = bar.get_height()\n",
        "            if pd.notna(yval) and yval > -1 : # Don't label if score was NaN\n",
        "                 plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.4f}', va='bottom', ha='center', fontsize=9)\n",
        "\n",
        "        plt.xticks(rotation=15, ha='right') # Rotate labels slightly if needed\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    print(\"\\nScript finished.\")\n",
        "    # Return the dictionary of results and perhaps the best models themselves\n",
        "    return best_models, test_results, scaler\n",
        "\n",
        "\n",
        "# --- Run the script ---\n",
        "if __name__ == \"__main__\":\n",
        "    # This might take some time depending on the data size and parameter grids\n",
        "    tuned_models, final_results, data_scaler = main()\n",
        "\n",
        "    # Example: Access the best Random Forest model if it was tuned\n",
        "    # if tuned_models and 'RandomForest' in tuned_models:\n",
        "    #     best_rf = tuned_models['RandomForest']\n",
        "    #     print(\"\\nBest Random Forest Model Parameters:\")\n",
        "    #     print(best_rf.get_params())\n",
        "        # # Make predictions with best_rf on new data (after scaling with data_scaler)\n",
        "        # # new_data_scaled = data_scaler.transform(new_raw_data)\n",
        "        # # predictions = best_rf.predict(new_data_scaled)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hydrogen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
